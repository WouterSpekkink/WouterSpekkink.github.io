I"Î<h2 id="intro">Intro</h2>
<p>This is going to be the first post in a series of posts on using the <a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a> framework.
With LangChain you can develop apps that are powered by Large Language Models (LLMs).
I primarily use LangChain to build applications for chatting about literature in my Zotero library and other text data on my computer.
My intention was to write a blog post that explains how to build these applications and how they work.
However, there is too much ground to cover for a single post, so I decided to break it down into multiple posts.</p>

<p>This first post covers:</p>
<ol>
  <li>Background on how the development of LLM-powered applications started for me.</li>
  <li>Application of LLM-powered that interests me most right now: <a href="https://www.promptingguide.ai/techniques/rag">retrieval augmentation</a></li>
  <li>Explanation of how I implement some of the steps in retrieval augmentation, like creating and maintaining <a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/">vector stores</a> for ingesting documents that we want to chat about. 
In future posts I will discuss other parts of the process, such as setting up chatbots using LangChain for discussing documents and creating <a href="https://python.langchain.com/docs/modules/agents/">agents</a> to take things further.</li>
</ol>

<h2 id="mind-blown">Mind-blown</h2>
<p>When ChatGPT was first released, I barely took notice.
I heard some people say impressive things about ChatGPT, but I didnâ€™t immediately feel the urge to try it out.
Eventually, a few months ago, I decided to give it a try.
I was mind-blown.
For an entire week, I had ChatGPT spit out crazy, nonsensical stories.
There was one about a talking horse that specialized in public-private partnerships and saved a village by helping to create new infrastructure.
I remember one about a hero who rode his horse backwards because he was afraid of being followed by purple frogs.
There was also another one about someone who put themselves in orbit around the Earth by pulling themselves up by their own hair.
The funniest part was that ChatGPT added a disclaimer by the end, stating that it was purely fictional and that you cannot actually put yourself in orbit in this way.</p>

<p>I quickly started trying out things that might be useful for my work in academia.
For example, I had ChatGPT come up with an assignment about analyzing policy interventions from a behavioral perspective.
Although I didnâ€™t actually use it, I could have with just a few tweaks.
I also entered into discussions with ChatGPT about theories and philosophy.
I found ChatGPT to be a useful conversational partner on many topics, as long as you already know your stuff and can spot the things that ChatGPT gets wrong (which happens frequently; at some point, I got fed up with ChatGPT constantly apologizing for getting things wrong).
I even tried a hybrid of storytelling and conversation on theories, having ChatGPT tell a fictional story about an academic and then querying ChatGPT about the contents of the papers written by this fictional academic.</p>

<p>I donâ€™t remember exactly when I started using ChatGPT for code writing, but its co-pilot capabilities are another aspect that blew me away and changed the way I write code. 
I recently read <a href="https://news.ycombinator.com/item?id=36855516">a post on hacker news</a> about how traffic on StackOverflow has declined recently. 
I have a strong suspicion that ChatGPT has contributed to this.</p>

<p>While I continue to be mind-blown to this day (in a positive way), I would also like to note that, like many others, I have occasionally felt uncertain and worried about where this will all lead. 
I am certainly no expert on AI, so please take anything I say on this with a grain of salt.
That being said, I am not that concerned about the â€˜AI going rogueâ€™ scenario, because I think that tools like ChatGPT give the strong appearance of being intelligent, but in reality are as dumb as a bag of rocks. 
What I am more afraid of is <a href="https://www.economist.com/by-invitation/2023/07/21/one-of-the-godfathers-of-ai-airs-his-concerns">what humans might do</a> with powerful tools like LLMs (or whatever comes next).
Also, I feel somewhat uncomfortable with the fact that progress in this area is driven almost entirely by business interests.
I think it is important that we think of alternative models for the further development of AI, such as the â€˜Islandâ€™ idea put forward in <a href="https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2">this article of the FT Magazine</a>.
It is also encouraging to see initiatives such as the development of <a href="https://huggingface.co/bigscience/bloomz">the Bloomz model</a> and <a href="https://github.com/bigscience-workshop/petals#benchmarks">petals</a> (my GPU is now hosting one block of a BloomZ model through petals; admittedly an almost meaningless contribution), which are both initiatives of <a href="https://bigscience.huggingface.co/">BigScience</a>. 
Yet, in my limited experience OpenAIâ€™s GPT models blow models such as Bloomz out of the water when it comes to the quality of their output.
The debate on how this AI revolution should unfold and be governed is an important one, but it is not a debate I want to engage with in these posts.
I would like to focus on different ways in which we can make LLMs useful for academic work.</p>

<blockquote>
I'll reiterate that I am no expert on AI, and given that many people who are an expert on the topic are worried, I am probably overlooking or misunderstanding something.
Feel free to write me a message to educate me on this.
</blockquote>

<h2 id="down-the-llm-rabbit-hole-with-langchain">Down the LLM rabbit hole with LangChain</h2>
<p>As described above, my first introduction to LLMs was through ChatGPT, which I believe is the case for many others.
While I had a lot of fun with ChatGPT alone, things became even more interesting after I discovered <a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a>.
I was introduced to LangChain by a <a href="https://www.youtube.com/watch?v=9AXP7tCI9PI">Youtube video</a>.
In the video, Techlead demonstrates how LangChain allows you to chat with Language Models (LLMs) about data stored on your own computer.
Techlead also provides a <a href="https://github.com/techleadhd/chatgpt-retrieval">simple example</a> on his GitHub repository, which can help you get started even if you donâ€™t fully understand how LangChain works.
As mentioned in the introduction, you can use LangChain to develop LLM-powered apps.
These LLMs can run on your own computer or be accessed via APIs.
The apps I have created using LangChain so far make use of the OpenAI API, which provides access to chat models like <code class="language-plaintext highlighter-rouge">gpt3.5-turbo</code> and <code class="language-plaintext highlighter-rouge">gpt4</code>, as well as the <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code> embedding model (Using the OpenAI API is <a href="https://openai.com/pricing">not for free</a>).</p>

<p>As the name suggests, LangChain utilizes chains, which <a href="https://python.langchain.com/docs/modules/chains/">the documentation</a> defines as sequences of calls to components. These components are abstractions for working with language models, along with various implementations for each abstraction. 
In simple terms, LangChain offers a set of tools that allow you to interact with LLMs in different ways, and it offers an easy way to chain these tools together.
This enables you to accomplish complex tasks with minimal code.
LangChain comes with a wide variety of pre-built chains, which means that you can build useful tools quickly.</p>

<p>LangChain also allows the creation of <a href="https://python.langchain.com/docs/modules/agents/">agents</a>, which are LLMs that can choose actions based on user prompts.
Agents simulate reasoning processes to determine which actions to take and in what order.
These actions often involve using <a href="https://python.langchain.com/docs/modules/agents/tools/">tools</a> that we provide to the agent, which are different types of chains powered by LLMs.
In simple terms, an agent is an LLM that can use other instances of LLMs for different tasks, depending on the specific needs.
There are undoubtedly many different kinds of useful applications that you can build with this framework, but I was drawn primarily to the idea of â€˜chatting with your own dataâ€™.
This involves something that is called <a href="https://www.promptingguide.ai/techniques/rag">retrieval augmentation</a>.</p>

<h2 id="retrieval-augmentation">Retrieval augmentation</h2>
<p>With retrieval augmentation, you extract information from documents and include that information as context in the messages that you send to an LLM. 
This allows the LLM to not only make use of the knowledge that it obtained during training (<a href="https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/">parametric knowledge</a>), but also of â€˜externalâ€™ knowledge that you extract from the documents (source knowledge).
Supposedly, this helps to combat so-called <a href="https://towardsdatascience.com/llm-hallucinations-ec831dcd7786">hallucinations</a> (or <a href="https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/">check this link</a> if you donâ€™t have a Medium account).
That in itself is useful, but I was primarily enthusiastic about the idea of chatting with an LLM about the literature that I have collected in my Zotero library.</p>

<blockquote>
Retrieval augmentation thus is something different from training or fine-tuning an LLM with your own data. It doesn't actually alter the model weights as would be the case with fine-tuning and training. Instead, the model just temporarily learns about the external knowledge included in your messages.
</blockquote>

<p>While the idea of extracting information from documents to include them as context in your messages to LLMs is simple enough, there are some challenges we need to overcome:</p>

<p>First, it is not practical if we have to manually find and extract the relevant information from our documents.
We might not even know exactly which information from which documents is relevant to our query in the first place.
Obviously, this part of the process is something we want to automate, which fortunately is easy using retrieval augmentation.</p>

<p>Second, there are limits to how much context we can include in our messages to LLMs.
Every LLM model has something called a context window, which refers to the number of tokens we can use in a single interaction with an LLM, including both the input (our query) and the output (the LLMâ€™s answer) of that interaction.
Different models have differently sized context windows.
For example, the <code class="language-plaintext highlighter-rouge">gpt3.5-turbo</code> model has a context window of 4,096 tokens.
The slightly more expensive <code class="language-plaintext highlighter-rouge">gpt3.5-turbo-16k</code> model, which I now use as my default, has a context window of 16,384 tokens.
The <code class="language-plaintext highlighter-rouge">gpt-4-32k</code> model has a context window of 32,768 tokens, but it is much more expensive than the <code class="language-plaintext highlighter-rouge">gpt3.5</code> models.
Anthropicâ€™s Claude 2, currently only available in the US and the UK, has an impressive context window size of 100k tokens!
Regardless, the length of the text that you include in your messages as context is limited by the modelâ€™s context window.
If we want to ask questions about our literature, we cannot simply dump our entire library of papers into our messages.</p>

<p>Third, we might not want to dump our entire library in our messages, or even an entire book or paper, for another reason: 
Not all information in a given paper will be relevant to the question we are asking the LLM. 
It would be preferable to include only the relevant bits of information in our messages and exclude anything that might distract from our question.
Fortunately, this can be easily achieved using tools provided by the LangChain framework.
I will now discuss some of these tools.</p>

<h2 id="vector-stores">Vector stores</h2>
<p><a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/">Vector stores</a> are perhaps the most important tools in the process of retrieval augmentation. 
A vector store is a kind of database in which you can store documents in two forms:</p>
<ol>
  <li>The actual documents in textual form, along with metadata.</li>
  <li>The documents in their â€˜embeddedâ€™ form, which is numerical representation of the documents.
In their embedded form, documents are stored as vectors that represent their position in a high-dimensional semantic space (the closer texts are in this space, the more similar they are in their meaning).
For example, OpenAIâ€™s <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code> model turns documents into vectors with 1,536 dimensions.</li>
</ol>

<p>LangChain supports a variety of vector stores.
The one I chose to use is the <a href="https://python.langchain.com/docs/integrations/vectorstores/faiss">FAISS</a> vector store, for the following reasons:</p>
<ol>
  <li>It allows you to keep your vector stores on your local drive (it doesnâ€™t require a cloud solution).</li>
  <li>For my purposes it is important that I can easily save, load and update a vector store and I found the approach that the FAISS vector store takes to this to be the most intuitive.</li>
</ol>

<p>Another type of vector store that offers similar functionality is ChromaDB, which also seems to be popular.
I advise you to <a href="https://js.langchain.com/docs/modules/data_connehttps://github.com/tesseract-ocr/tesseract/tree/0768e4ff4c21aaf0b9beb297e6bb79ad8cb301b0ction/vectorstores/">explore</a> the different available types of vector stores in LangChain before picking one to use yourself.</p>

<p>To store our documents in a vector store we need to take multiple steps (Iâ€™ll walk through these in more detail in the remainder of this blog post):</p>
<ol>
  <li>We need to convert our documents to plain text (assuming that many of them will be in PDF-format originally).
LangChain includes PyPDF-based tools that will do this for you, but I opted to convert my files using a bash script that utilizes the <code class="language-plaintext highlighter-rouge">pdfttotext</code> and <code class="language-plaintext highlighter-rouge">pdfimage</code> command-line tools. 
You might want to simply make use of the built-in tools that LangChain provides instead. 
I opted for the bash script because it makes it easier to check the results of the conversion process.</li>
  <li>We need to load our documents into our application, for which LangChain again offers multiple solutions.
Weâ€™ll use the <code class="language-plaintext highlighter-rouge">DirectoryLoader</code> as weâ€™ll be loading multiple documents from a directory.</li>
  <li>We might want to add metadata to our documents, which can be stored along with the documents in our vector store. 
For example, I like to add the bibliographical details of the publications in my Zotero library.
We will want to do this before we cut up our documents in smaller chunks (the next step).</li>
  <li>We will want to cut our documents into smaller chunks that we than store separately in our vector store.
When we retrieve information from our vector store, weâ€™ll thus retrieve these smaller chunks, rather than the entire original documents.
This allows us to retrieve relevant information in a more targeted way, as well as limit the amount of text we include as context in our messages to LLMs (see our earlier discussion on challenges in extracting contextual information from documents).
LangChain comes with multiple text splitters that can accomplish this task for us.
Weâ€™ll be using the <code class="language-plaintext highlighter-rouge">RecursiveCharacterTextSplitter</code>.</li>
  <li>We need to create the embeddings for our chunks of texts, for which we will use OpenAIâ€™s <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code>.
Again, there are other options available, but I havenâ€™t had the chance to experiment with these yet and Iâ€™m quite happy with the results I have achieved with the OpenAI solution.</li>
  <li>Then we have everything we need to store our documents, along with their embeddings, in our vector store.</li>
</ol>

<p>Letâ€™s now go through these steps in more detail.</p>

<h2 id="converting-to-text">Converting to text</h2>
<p>Iâ€™ll briefly explain the logic of the bash script (which you can find below) that I use to convert the literature in my Zotero library (all PDFs) to text.</p>

<p>The bash script finds all the PDFs included in my Zotero storage folder, which has sub-folders for each publication.
For each file it checks if the filename is already mentioned in a text file that I use to keep track of every document that I have already ingested.
I frequently update my Zotero library, and if I want update my vector store by adding new publications, I donâ€™t want to also convert all files that have already been ingested. Keeping track of the files that have already been ingested allows me to skip these in the conversion process.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c"># One file to keep the papers that I have already ingested</span>
<span class="c"># One dir to store newly added papers</span>
<span class="c"># A temporary dir for image-based pdfs.</span>
<span class="nv">existing_file</span><span class="o">=</span><span class="s2">"/home/wouter/Documents/LangChain_Projects/Literature/data/ingested.txt"</span>
<span class="nv">output_dir</span><span class="o">=</span><span class="s2">"/home/wouter/Documents/LangChain_Projects/Literature/data/new"</span>
<span class="nv">temp_dir</span><span class="o">=</span><span class="s2">"/home/wouter/Documents/LangChain_Projects/Literature/data/temp"</span>

<span class="nv">counter</span><span class="o">=</span>0

<span class="nv">total</span><span class="o">=</span><span class="si">$(</span>find /home/wouter/Tools/Zotero/storage/ <span class="nt">-type</span> f <span class="nt">-name</span> <span class="s2">"*.pdf"</span> | <span class="nb">wc</span> <span class="nt">-l</span><span class="si">)</span>

find /home/wouter/Tools/Zotero/storage <span class="nt">-type</span> f <span class="nt">-name</span> <span class="s2">"*.pdf"</span> | <span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> file
<span class="k">do
    </span><span class="nv">base_name</span><span class="o">=</span><span class="si">$(</span><span class="nb">basename</span> <span class="s2">"</span><span class="nv">$file</span><span class="s2">"</span> .pdf<span class="si">)</span>

    <span class="k">if </span><span class="nb">grep</span> <span class="nt">-Fxq</span> <span class="s2">"</span><span class="nv">$base_name</span><span class="s2">.txt"</span> <span class="s2">"</span><span class="nv">$existing_file</span><span class="s2">"</span><span class="p">;</span> <span class="k">then
	</span><span class="nb">echo</span> <span class="s2">"Text file for </span><span class="nv">$file</span><span class="s2"> already exists, skipping."</span>
    <span class="k">else 
	</span>pdftotext <span class="nt">-enc</span> UTF-8 <span class="s2">"</span><span class="nv">$file</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$output_dir</span><span class="s2">/</span><span class="nv">$base_name</span><span class="s2">.txt"</span>

	pdfimages <span class="s2">"</span><span class="nv">$file</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$temp_dir</span><span class="s2">/</span><span class="nv">$base_name</span><span class="s2">"</span>
	
    <span class="k">fi
    </span><span class="nv">counter</span><span class="o">=</span><span class="k">$((</span>counter <span class="o">+</span> <span class="m">1</span><span class="k">))</span>
    <span class="nb">echo</span> <span class="nt">-ne</span> <span class="s2">"Processed </span><span class="nv">$counter</span><span class="s2"> out of </span><span class="nv">$total</span><span class="s2"> PDFs.</span><span class="se">\r</span><span class="s2">"</span>
    
<span class="k">done</span>
</code></pre></div></div>
<p>I have the bash script convert all PDFs to text with <code class="language-plaintext highlighter-rouge">pdftotext</code>, but I also convert the same files with <code class="language-plaintext highlighter-rouge">pdfimages</code>, since some of the PDFs have images rather than text (the PDFs where you cannot select the text in a regular PDF reader).
The images are stored in a temporary folder.
After converting the files, I basically just inspect the resulting files and try to identify files that <code class="language-plaintext highlighter-rouge">pdftotext</code> was not able to convert successfully (usually these are just a few bytes in size).
For all files that <em>were</em> converted successfully I delete the image files in the temporary folder.
The remaining image files are converted with another bash script (shown below), which makes use of <a href="https://github.com/tesseract-ocr/tesseract/tree/0768e4ff4c21aaf0b9beb297e6bb79ad8cb301b0">tesseract</a>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="nv">output_dir</span><span class="o">=</span><span class="s2">"/home/wouter/Documents/LangChain_Projects/Literature/data/new/"</span>
<span class="nv">pbm_directory</span><span class="o">=</span><span class="s2">"/home/wouter/Documents/LangChain_Projects/Literature/data/temp"</span>

<span class="c"># Create an associative array</span>
<span class="nb">declare</span> <span class="nt">-A</span> base_names

<span class="c"># Handle filenames with spaces by changing the Internal Field Separator (IFS)</span>
<span class="nv">oldIFS</span><span class="o">=</span><span class="s2">"</span><span class="nv">$IFS</span><span class="s2">"</span>
<span class="nv">IFS</span><span class="o">=</span><span class="s1">$'</span><span class="se">\n</span><span class="s1">'</span>

<span class="c"># Go through each file in the PBM directory</span>
<span class="k">for </span>file <span class="k">in</span> <span class="s2">"</span><span class="nv">$pbm_directory</span><span class="s2">"</span>/<span class="k">*</span>.pbm <span class="s2">"</span><span class="nv">$pbm_directory</span><span class="s2">"</span>/<span class="k">*</span>.ppm
<span class="k">do</span>
    <span class="c"># Get the base name from the path</span>
    <span class="nv">base_name</span><span class="o">=</span><span class="si">$(</span><span class="nb">basename</span> <span class="s2">"</span><span class="nv">$file</span><span class="s2">"</span> | rev | <span class="nb">cut</span> <span class="nt">-d-</span> <span class="nt">-f2-</span> | rev<span class="si">)</span>

    <span class="c"># Add the base name to the associative array</span>
    base_names[<span class="s2">"</span><span class="nv">$base_name</span><span class="s2">"</span><span class="o">]=</span>1
<span class="k">done</span>

<span class="c"># Restore the original IFS</span>
<span class="nv">IFS</span><span class="o">=</span><span class="s2">"</span><span class="nv">$oldIFS</span><span class="s2">"</span>

<span class="c"># Go through each unique base name</span>
<span class="k">for </span>base_name <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="p">!base_names[@]</span><span class="k">}</span><span class="s2">"</span>
<span class="k">do</span>
    <span class="c"># Remove any existing text file for this base name</span>
    <span class="nb">rm</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$output_dir</span><span class="s2">/</span><span class="nv">$base_name</span><span class="s2">.txt"</span>

    <span class="c"># Go through each PBM file for this base name, handling spaces in filenames</span>
    <span class="k">for </span>ext <span class="k">in </span>pbm ppm
    <span class="k">do
        </span>find <span class="s2">"</span><span class="nv">$pbm_directory</span><span class="s2">"</span> <span class="nt">-name</span> <span class="s2">"</span><span class="nv">$base_name</span><span class="s2">-*.</span><span class="nv">$ext</span><span class="s2">"</span> <span class="nt">-print0</span> | <span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> <span class="nt">-d</span> <span class="s1">$'</span><span class="se">\0</span><span class="s1">'</span> file
        <span class="k">do</span>
            <span class="c"># OCR the file and append the results to the text file</span>
	    <span class="nb">echo</span> <span class="s2">"Converting </span><span class="nv">$file</span><span class="s2">"</span> 
            tesseract <span class="s2">"</span><span class="nv">$file</span><span class="s2">"</span> stdout <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$output_dir</span><span class="s2">/</span><span class="nv">$base_name</span><span class="s2">.txt"</span>
        <span class="k">done
    done
done</span>
</code></pre></div></div>
<p>After doing this, I have all documents stored as plain text files in one folder.</p>

<h2 id="continuing-in-python">Continuing in Python</h2>
<p>The remaining steps that I discuss in this post are all implemented in Python.
I will share a Python script that I call <code class="language-plaintext highlighter-rouge">indexer.py</code>, which I use to create a new vector store for the literature in my Zotero library.
Iâ€™ll be going through the script in steps.
Letâ€™s start with some basic â€˜housekeepingâ€™ stuff, such as imports, loading our OpenAI API Key (without it, we cannot use the OpenAI models), and setting some paths that weâ€™ll be using throughout the script.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="n">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span><span class="p">,</span> <span class="n">TextLoader</span>
<span class="kn">import</span> <span class="n">langchain</span>
<span class="kn">import</span> <span class="n">bibtexparser</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="n">openai</span>
<span class="kn">import</span> <span class="n">constants</span>

<span class="c1"># Set OpenAI API Key
</span><span class="nf">load_dotenv</span><span class="p">()</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">OPENAI_API_KEY</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">constants</span><span class="p">.</span><span class="n">APIKEY</span>
<span class="n">openai</span><span class="p">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">constants</span><span class="p">.</span><span class="n">APIKEY</span> 

<span class="c1"># Set paths
</span><span class="n">source_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./data/new/</span><span class="sh">'</span>
<span class="n">store_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./vectorstore/</span><span class="sh">'</span>
<span class="n">destination_file</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./data/ingested.txt</span><span class="sh">'</span>
<span class="n">bibtex_file_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">/home/wouter/Tools/Zotero/bibtex/library.bib</span><span class="sh">'</span>
</code></pre></div></div>
<p>Youâ€™ll notice that I import my API key from a file called <code class="language-plaintext highlighter-rouge">constants.py</code>, which is a file that just defines one variable, called <code class="language-plaintext highlighter-rouge">APIKEY</code>, which is a string that contains my API key. 
If you donâ€™t have an OpenAI API key yet, you can make one on the <a href="https://platform.openai.com/">OpenAI platform</a>.
It is important that you donâ€™t share your API key with anyone.</p>

<p>In the snippet of Python code above we set a couple of paths:</p>
<ul>
  <li>The <code class="language-plaintext highlighter-rouge">source_path</code> which contains all the text files we created in the previous step.</li>
  <li>The <code class="language-plaintext highlighter-rouge">store_path</code> where we will keep our vector store.</li>
  <li>The <code class="language-plaintext highlighter-rouge">destination_file</code> to which weâ€™ll write the names of all the files weâ€™ve successfully ingested later on.</li>
  <li>The <code class="language-plaintext highlighter-rouge">bibtex_file_path</code> where we store our Zotero-generated bibtex file.
We will access this file to retrieve the bibliographical metadata that we want to include with our documents.</li>
</ul>

<h2 id="loading-our-documents-and-adding-metadata">Loading our documents and adding metadata</h2>
<p>The next step is to actually load our documents, which we can easily accomplish with LangChainâ€™s <a href="https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory">DirectoryLoader</a>.
Before chunking our documents we will also want to add the metadata to them, so that the metadata is associated with the relevant chunks.</p>

<p>We simply set up our <code class="language-plaintext highlighter-rouge">DirectoryLoader</code>, passing our <code class="language-plaintext highlighter-rouge">source_path</code> as its first argument and then setting a few options that help ensure a smooth process (the <code class="language-plaintext highlighter-rouge">show_progress=True</code> argument is not strictly necessary).</p>

<p>To add our metadata, we can go through our bibtex file, using the <a href="https://bibtexparser.readthedocs.io/en/main/">bibtexparser</a> library, and weâ€™ll match the names of our documents to the filenames recorded in the bibtex file (Zotero conveniently records these names along with the bibliographical details).
After extracting the metadata, we go through our list of the imported documents, and we add the metadata.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load documents
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Loading documents===</span><span class="sh">"</span><span class="p">)</span>
<span class="n">loader</span> <span class="o">=</span> <span class="nc">DirectoryLoader</span><span class="p">(</span><span class="n">source_path</span><span class="p">,</span>
                         <span class="n">show_progress</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                         <span class="n">use_multithreading</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                         <span class="n">loader_cls</span><span class="o">=</span><span class="n">TextLoader</span><span class="p">,</span>
                         <span class="n">loader_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">autodetect_encoding</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="c1"># Add metadata based in bibliographic information
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Adding metadata===</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Read the BibTeX file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">bibtex_file_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">bibtex_file</span><span class="p">:</span>
    <span class="n">bib_database</span> <span class="o">=</span> <span class="n">bibtexparser</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">bibtex_file</span><span class="p">)</span>

<span class="c1"># Get a list of all text file names in the directory
</span><span class="n">text_file_names</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">source_path</span><span class="p">)</span>
<span class="n">metadata_store</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Go through each entry in the BibTeX file
</span><span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">bib_database</span><span class="p">.</span><span class="n">entries</span><span class="p">:</span>
    <span class="c1"># Check if the 'file' key exists in the entry
</span>    <span class="k">if</span> <span class="sh">'</span><span class="s">file</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">entry</span><span class="p">:</span>
        <span class="c1"># Extract the file name from the 'file' field and remove the extension
</span>        <span class="n">pdf_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">entry</span><span class="p">[</span><span class="sh">'</span><span class="s">file</span><span class="sh">'</span><span class="p">]).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">.pdf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">)</span>

         <span class="c1"># Check if there is a text file with the same name
</span>        <span class="k">if</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">pdf_file_name</span><span class="si">}</span><span class="s">.txt</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">text_file_names</span><span class="p">:</span>
            <span class="c1"># If a match is found, append the metadata to the list
</span>            <span class="n">metadata_store</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>

<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">metadata_store</span><span class="p">:</span>
        <span class="n">doc_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">document</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">)</span>
        <span class="n">ent_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">entry</span><span class="p">[</span><span class="sh">'</span><span class="s">file</span><span class="sh">'</span><span class="p">]).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">.pdf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">doc_name</span> <span class="o">==</span> <span class="n">ent_name</span><span class="p">:</span>
            <span class="n">document</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
</code></pre></div></div>

<p>Just for reference, I include an example of what a bibtex entry in my bibtex files looks like.</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">Abbott1984</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Event Sequence and Event Duration: Colligation and Measurement [in Medicine].}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Event Sequence and Event Duration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Abbott, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{1984}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Historical methods}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{11620185}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{pubmed}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{192--204}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0161-5440}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1080/01615440.1984.10594134}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{0161-5440}</span><span class="p">,</span>
  <span class="na">pmid</span> <span class="p">=</span> <span class="s">{11620185}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Historiography,History of Medicine,History- Ancient,History- Early Modern 1451-1600,History- Medieval,History- Modern 1601-,Medicine,United States}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{/home/wouter/Tools/Zotero/storage/7BM53SZ6/Abbott1984.pdf}</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="splitting-the-documents">Splitting the documents</h2>

<p>Now that we have our documents, including metadata, we can go on and split them into chunks.
As mentioned previously, we can use the <a href="https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory">RecursiveCharacterTextSplitter</a> for this, which is very good at splitting texts into chunks of the size that we desire, while keeping semantically meaningful structures (e.g., paragraphs) intact as much as possible.</p>

<p>We need to decide what the size of our chunks will be.
I believe a popular choice is to go with chunks of 1000 tokens.
I opted for 1500 tokens because it just slightly increases the chances that parts of the text that belong together also end up in chunks together.
We can also set an overlap for our chunks, which I set to 150.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Splitting text
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Splitting documents into chunks===</span><span class="sh">"</span><span class="p">)</span>
<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">1500</span><span class="p">,</span>
    <span class="n">chunk_overlap</span>  <span class="o">=</span> <span class="mi">150</span><span class="p">,</span>
    <span class="n">length_function</span> <span class="o">=</span> <span class="nb">len</span><span class="p">,</span>
    <span class="n">add_start_index</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">split_documents</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="embedding-the-documents-and-creating-our-vector-store">Embedding the documents and creating our vector store</h2>
<p>The final steps are to create embeddings for our chunks of texts and then store them, alongside the chunks themselves, in our vector store.
These embeddings are what we actually use later when we want to retrieve information from our vector store (discussed in more detail in a future post).
Basically, when we ask our LLM a question, the question will be embedded as well and its vectorized form will then be used to find entries in our vector store that are similar in meaning.
This approach to finding relevant information is much more accurate than finding relevant information purely based on matches between the texts themselves.</p>

<blockquote>
One cool benefit of storing documents in their vectorized form is that the language in which the documents were written becomes less relevant. 
Two documents that are written in different languages, but have similar meanings, will end up in similar positions in the semantic space when they are embedded.
</blockquote>

<p>As mentioned previously, we use the <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code> model to create our embeddings. 
This is the default model when using LangChainâ€™s <code class="language-plaintext highlighter-rouge">OpenAIEmbeddings()</code> function.</p>

<p>Creating the embeddings is the most time consuming part of this process. 
I started out with a library of about 1750 documents (before chunking), which I believe takes about an hour to complete the embeddings for (this is a guess, because I didnâ€™t consciously keep track of time).
It is also a relatively expensive part of the process, since weâ€™ll be sending a lot of tokens through the OpenAI API.
This is one of the reasons why it is useful to have a setup where you donâ€™t have to recreate these embeddings repeatedly (see the comments on updating our vector store by the end of this post).</p>

<p>You will probably also frequently see warnings about hitting OpenAIâ€™s rate limits. 
Fortunately, LangChain has built-in functions that delay further requests until weâ€™re ready to resume the process, so we donâ€™t need to worry about this.</p>

<p>After the embeddings have been created, you can create your vector store as shown in the snippet. 
We immediately save our vector store in the folder that we defined for it earlier.</p>

<p>The last thing that we do is to write the filenames of the ingested documents to the file that we use to keep track of all ingested documents, allowing us to skip these when updating the vector store.</p>

<p>After the script finishes its run, I manually delete the text files from the folder from which we sourced them.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Embedding documents
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Embedding text and creating database===</span><span class="sh">"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">(</span>
    <span class="n">show_progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">request_timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">FAISS</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">split_documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">db</span><span class="p">.</span><span class="nf">save_local</span><span class="p">(</span><span class="n">store_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Record what we have ingested
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Recording ingested files===</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">destination_file</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">document</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]))</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="updating-the-vector-store">Updating the vector store</h2>
<p>As mentioned above, creating embeddings for documents is relatively expensive, both in terms of time and in terms of actual money spent on using the OpenAI API.
Therefore, we do not want to create embeddings for any given document more than once.
I already explained how the bash script that I use to convert PDFs skips documents that weâ€™ve already ingested.
If I add new papers to my Zotero library, and I run the conversion script, only the PDFs of the newly added papers will be converted and eventually end up in the folder from which we source the documents to be ingested in the vector store.</p>

<p>To add these new papers to my existing vector store, I use a script that I named <code class="language-plaintext highlighter-rouge">updater.py</code> (see below).
This script is identical to the <code class="language-plaintext highlighter-rouge">indexer.py</code> script, except for the last part, where I:</p>
<ol>
  <li>create a new vector store to ingest the new papers,</li>
  <li>load the existing vector store that I initially created with the <code class="language-plaintext highlighter-rouge">indexer.py</code> script,</li>
  <li>merge these two vector stores, and</li>
  <li>store the merged vector store to my disk, overwriting the original one.</li>
</ol>

<p>This process requires me to create embeddings only for new papers that I added to my library.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Embedding text and creating database===</span><span class="sh">"</span><span class="p">)</span>
<span class="n">new_db</span> <span class="o">=</span> <span class="n">FAISS</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">split_documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Merging new and old database===</span><span class="sh">"</span><span class="p">)</span>
<span class="n">old_db</span> <span class="o">=</span> <span class="n">FAISS</span><span class="p">.</span><span class="nf">load_local</span><span class="p">(</span><span class="n">store_path</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">old_db</span><span class="p">.</span><span class="nf">merge_from</span><span class="p">(</span><span class="n">new_db</span><span class="p">)</span>
<span class="n">old_db</span><span class="p">.</span><span class="nf">save_local</span><span class="p">(</span><span class="n">store_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Record the files that we have added
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">===Recording ingested files===</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">destination_file</span><span class="p">,</span> <span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">document</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]))</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="outlook-to-future-posts">Outlook to future posts</h2>
<p>This is all that I wanted to share in this particular post.
What we have done now is to create a vector store that includes (for example) literature in our Zotero library, which allows us to then use that literature as context in chat sessions with LLMs.
How we actually set up these chat sessions and how we can use the vector stores in them is something I will discuss in a future post.</p>

:ET